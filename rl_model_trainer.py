import os
import shutil
import pandas as pd
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from rl_environment import PortfolioTradingEnv


class RLModelTrainer:
    """
    ê°•í™”í•™ìŠµ PPO ì—ì´ì „íŠ¸ í›ˆë ¨ ë° ë¡œë“œë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    """

    def __init__(
        self,
        model_path="trading_agent.zip",
        tensorboard_log_path="./rl_tensorboard_logs/",
    ):
        self.model_path = model_path
        self.tensorboard_log_path = tensorboard_log_path

    def train_agent(self, total_timesteps=100_000, ticker="BTC/KRW"):
        """
        PPO ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤.
        """
        if os.path.exists(self.tensorboard_log_path):
            print(f"ê¸°ì¡´ ë¡œê·¸ ë””ë ‰í† ë¦¬ {self.tensorboard_log_path}ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
            shutil.rmtree(self.tensorboard_log_path)
        os.makedirs(self.tensorboard_log_path, exist_ok=True)

        print(f"ğŸ¤– {ticker}ì— ëŒ€í•œ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        # 1. ë°ì´í„° ë¡œë“œ
        cache_dir = "cache"
        file_path = os.path.join(cache_dir, f"{ticker.replace('/', '_')}_1h.feather")
        if not os.path.exists(file_path):
            print(f"ì˜¤ë¥˜: {file_path}ì—ì„œ í›ˆë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € 'preprocess' ëª¨ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
            return

        df = pd.read_feather(file_path)
        df.set_index("timestamp", inplace=True)

        df.drop(columns=["regime"], inplace=True, errors="ignore")
        df.dropna(inplace=True)
        df = df.astype(np.float32)

        if len(df) < 200:
            print(
                f"ì˜¤ë¥˜: {ticker}ì˜ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ í›ˆë ¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({len(df)} rows)."
            )
            return

        # 2. ê±°ë˜ í™˜ê²½ ìƒì„±
        env = TradingEnv(df)
        vec_env = DummyVecEnv([lambda: env])

        # 3. PPO ëª¨ë¸ ì •ì˜
        model = PPO(
            "MlpPolicy", vec_env, verbose=1, tensorboard_log=self.tensorboard_log_path
        )

        # 4. ëª¨ë¸ í›ˆë ¨
        print(f"ì´ {total_timesteps} íƒ€ì„ìŠ¤í… ë™ì•ˆ í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
        model.learn(total_timesteps=total_timesteps)

        # 5. ëª¨ë¸ ì €ì¥
        model.save(self.model_path)
        print(f"âœ… í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ '{self.model_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def load_agent(self):
        """
        ì €ì¥ëœ PPO ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        if os.path.exists(self.model_path):
            print(f"ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ë¥¼ '{self.model_path}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.")
            return PPO.load(self.model_path)
        else:
            print(
                f"ê²½ê³ : ì €ì¥ëœ ì—ì´ì „íŠ¸ íŒŒì¼('{self.model_path}')ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            return None


if __name__ == "__main__":
    # ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°, í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.
    trainer = RLModelTrainer()
    trainer.train_agent(total_timesteps=200_000)
